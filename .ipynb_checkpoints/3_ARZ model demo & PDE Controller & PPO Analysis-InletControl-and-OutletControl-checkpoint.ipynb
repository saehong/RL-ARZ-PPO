{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settins of dt:0.2\n",
      "Settins of dx:12.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import gym_arz\n",
    "from settings_file import *\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARZ model simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1: Outlet Boundary Control\n",
      "Determinstic Env.\n",
      "Initial condition of rs: 0.12\n",
      "Action space is discrete?: False\n",
      "Size of actions: Box(1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saehong/miniconda3/envs/shp_1/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"arz-v0\", sett=settings, cont_sett = control_settings)\n",
    "\n",
    "print(\"Action space is discrete?:\", env.discrete)\n",
    "print(\"Size of actions: {}\".format(env.action_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_size = len(env.r)\n",
    "y_size = len(env.y)\n",
    "v_size = len(env.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter values from the Gym environment\n",
    "\n",
    "# Parameter\n",
    "vm = settings['vm']#40\n",
    "rm = settings['rm']#0.16\n",
    "tau = settings['tau']#60\n",
    "L = settings['L']#500\n",
    "T = settings['T'] #settings['T'] is 120 (2mins)\n",
    "vs = settings['vs']#10\n",
    "rs  = settings['rs']#0.12\n",
    "qs  = settings['qs']#rs * vs\n",
    "gam = settings['gam']#1\n",
    "ps  = vm/rm * qs/vs\n",
    "ys  = 0;\n",
    "\n",
    "\n",
    "# Discretization\n",
    "dx = settings['dx']#5\n",
    "dt = settings['dt']#0.1\n",
    "t = np.arange(0,T+dt,dt)\n",
    "x = np.arange(0,L+dx,dx)\n",
    "M = len(x)\n",
    "N = len(np.arange(0,T+dt,dt))\n",
    "\n",
    "# charateristics\n",
    "lambda_1 = vs ;\n",
    "lambda_2 = vs - rs * vm/rm ;\n",
    "\n",
    "# Fundamental diagram\n",
    "Veq = lambda rho: vm * ( 1 - rho/rm)\n",
    "\n",
    "# Flux\n",
    "F_r = lambda rho,y: y + rho * Veq(rho)\n",
    "F_y = lambda rho,y: y * (y/rho + Veq(rho))\n",
    "\n",
    "# Spatial function\n",
    "c_x = lambda x: -1 / tau * np.exp(-x/tau/vs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation setting\n",
    "\n",
    "r_save_base = np.zeros([r_size,N])\n",
    "y_save_base = np.zeros([y_size,N])\n",
    "v_save_base = np.zeros([v_size,N])\n",
    "action_save_base = np.zeros([2,N])\n",
    "reward_save_base = np.zeros(N)\n",
    "\n",
    "env.reset()\n",
    "r_save_base[:,0] = env.r.reshape(r_size,)\n",
    "y_save_base[:,0] = env.y.reshape(y_size,)\n",
    "v_save_base[:,0] = env.v.reshape(v_size,)\n",
    "action_save_base[:,0] = 0\n",
    "\n",
    "\n",
    "\n",
    "rs = env.rs#0.12\n",
    "vs = env.vs#10\n",
    "qs = env.qs\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "# Find input index close to qs_input\n",
    "env_qs_input = env.qs_input\n",
    "idx = find_nearest(env_qs_input,qs)\n",
    "\n",
    "DISCRETE = env.discrete\n",
    "\n",
    "for i in range(N-1):\n",
    "    #action = find_nearest(env_qs_input,qs)\n",
    "    if DISCRETE:\n",
    "        action = find_nearest(env_qs_input,qs)\n",
    "    else:\n",
    "        action = np.array([qs,qs])\n",
    "    states, reward, is_done, info = env.step(action)\n",
    "    r_save_base[:,i+1] = env.r.reshape(r_size,)\n",
    "    y_save_base[:,i+1] = env.y.reshape(y_size,)\n",
    "    v_save_base[:,i+1] = env.v.reshape(v_size,)\n",
    "    action_save_base[:,i+1] = action\n",
    "    reward_save_base[i+1] = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# ax.plot3D(xline, yline, zline, 'gray')\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L']  #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [min]')\n",
    "ax.set_zlabel('Density [veh/km]')\n",
    "ax.set_title(r'$\\rho$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],r_save_base[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=1,antialiased=True,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],r_save_base[:,0],color='blue',LineWidth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L'] #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [min]')\n",
    "ax.set_zlabel('Velocity [m/s]')\n",
    "ax.set_title(r'$V$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],v_save_base[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=0.5,antialiased=True,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],v_save_base[:,0],color='blue',LineWidth=4)\n",
    "\n",
    "\n",
    "# ax.plot_surface(xx,tt,v_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we save the results in mat.file and plot them, they are looking better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "plt.plot(t[1:],reward_save_base[1:])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(reward_save_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_save_base = r_save_base * v_save_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_save_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "plt.plot(t[1:],q_save_base[-2,1:])\n",
    "plt.ylabel('q(L-1,t)')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "plt.plot(q_save_base[-1,1:])\n",
    "plt.ylabel('q(L,t), applied input')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.plot(q_save_base[0,:])\n",
    "plt.ylabel('q(0,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert python data into Matlab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('ARZ_python_result_Base_Stochastic_rs012.mat',{'r_vec_base':r_save_base, 'v_vec_base':v_save_base, 'rwd_base' : reward_save_base, 'input_base': action_save_base, 'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interacting with env.\n",
    "\n",
    "env.reset()\n",
    "q_inlet_save = np.zeros([1,N])\n",
    "r_save = np.zeros([r_size,N])\n",
    "y_save = np.zeros([y_size,N])\n",
    "v_save = np.zeros([v_size,N])\n",
    "PDE_input_save = np.zeros([1,N])\n",
    "reward_save = np.zeros(N)\n",
    "\n",
    "r_save[:,0] = env.r.reshape(r_size,)\n",
    "y_save[:,0] = env.y.reshape(y_size,)\n",
    "v_save[:,0] = env.v.reshape(v_size,)\n",
    "\n",
    "v_temp = y_save[:,0]/r_save[:,0] + Veq(r_save[:,0]) - vs * np.ones((M,))\n",
    "v_temp = v_temp.reshape(v_temp.shape[0],1) # flatten -> column vector\n",
    "q_temp = y_save[:,0] + r_save[:,0] * Veq(r_save[:,0]) - qs * np.ones((M,))\n",
    "q_temp = q_temp.reshape(q_temp.shape[0],1) # flatten -> column vector\n",
    "\n",
    "Trans_K =  np.transpose(K[:,0]).reshape(1,K[:,0].shape[0])\n",
    "IM = np.fliplr(-Trans_K) * np.transpose(v_temp)\n",
    "\n",
    "IK_v_left = lambda_2 / lambda_1 * K[M-1,:] * np.exp(x/tau/vs)\n",
    "IK_v_left = IK_v_left.reshape(1,IK_v_left.shape[0]) # flatten -> row vector\n",
    "IK_v = IK_v_left * np.transpose(v_temp)\n",
    "IK_q_left = (lambda_1 - lambda_2) / qs * K[M-1,:] * np.exp(x/tau/vs)\n",
    "IK_q_left = IK_q_left.reshape(1,IK_q_left.shape[0]) # flatten -> row vector\n",
    "IK_q = IK_q_left * np.transpose(q_temp)\n",
    "\n",
    "U = np.zeros(N)\n",
    "U[0] = np.trapz(IM,x=x) + np.trapz(IK_v,x=x) + np.trapz(IK_q,x=x)\n",
    "\n",
    "qs_input = (U[0]+vs)*r_save[-1,0]\n",
    "\n",
    "\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "# Find input index close to qs_input\n",
    "env_qs_input = env.qs_input\n",
    "idx = find_nearest(env_qs_input,qs_input)\n",
    "\n",
    "for i in range(N-1):\n",
    "    #action = find_nearest(env_qs_input,qs_input)\n",
    "    #q_inlet_save[:,i+1] = env.q_inlet.reshape(1,)\n",
    "    if DISCRETE:\n",
    "        action = find_nearest(env_qs_input,qs)\n",
    "    else:\n",
    "        action = qs_input\n",
    "        PDE_input_save[:,i] = qs_input\n",
    "    states, reward, is_done, info = env.step(action)\n",
    "    r_save[:,i+1] = env.r.reshape(r_size,)\n",
    "    y_save[:,i+1] = env.y.reshape(y_size,)\n",
    "    v_save[:,i+1] = env.v.reshape(v_size,)\n",
    "    reward_save[i+1] = reward\n",
    "    \n",
    "    # Closed-loop control\n",
    "    v_temp = y_save[:,i+1]/r_save[:,i+1] + Veq(r_save[:,i+1]) - vs * np.ones((M,))\n",
    "    v_temp = v_temp.reshape(v_temp.shape[0],1) # flatten -> column vector\n",
    "    q_temp = y_save[:,i+1] + r_save[:,i+1] * Veq(r_save[:,i+1]) - qs * np.ones((M,))\n",
    "    q_temp = q_temp.reshape(q_temp.shape[0],1) # flatten -> column vector\n",
    "\n",
    "    Trans_K =  np.transpose(K[:,0]).reshape(1,K[:,0].shape[0])\n",
    "    IM = np.fliplr(-Trans_K) * np.transpose(v_temp)\n",
    "\n",
    "    IK_v_left = lambda_2 / lambda_1 * K[M-1,:] * np.exp(x/tau/vs)\n",
    "    IK_v_left = IK_v_left.reshape(1,IK_v_left.shape[0]) # flatten -> row vector\n",
    "    IK_v = IK_v_left * np.transpose(v_temp)\n",
    "    IK_q_left = (lambda_1 - lambda_2) / qs * K[M-1,:] * np.exp(x/tau/vs)\n",
    "    IK_q_left = IK_q_left.reshape(1,IK_q_left.shape[0]) # flatten -> row vector\n",
    "    IK_q = IK_q_left * np.transpose(q_temp)\n",
    "\n",
    "    U[i+1] = np.trapz(IM,x=x) + np.trapz(IK_v,x=x) + np.trapz(IK_q,x=x)\n",
    "\n",
    "    qs_input = (U[i+1]+vs)*r_save[-1,i+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_save_bcmk = r_save\n",
    "y_save_bcmk = y_save\n",
    "v_save_bcmk = v_save\n",
    "reward_save_bcmk = reward_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L']  #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [min]')\n",
    "ax.set_zlabel('Density [veh/km]')\n",
    "ax.set_title(r'$\\rho$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],r_save_bcmk[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=1,antialiased=True,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],r_save_bcmk[:,0],color='blue',LineWidth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L'] #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [min]')\n",
    "ax.set_zlabel('Velocity [m/s]')\n",
    "ax.set_title(r'$V$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],v_save_bcmk[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=1,antialiased=True,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],v_save_bcmk[:,0],color='blue',LineWidth=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "\n",
    "plt.plot(t,reward_save)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(reward_save_bcmk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the input profile here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_save_bcmk = r_save_bcmk * v_save_bcmk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_inlet_save[0,1:])\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_bcmk[-1,:])\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_bcmk[0,:])\n",
    "plt.ylabel('q(0,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(PDE_input_save[0,:-1])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.title('INPUT SHAPE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data for PLOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('ARZ_python_result_BCMK.mat',{'r_vec_bcmk':r_save_bcmk, 'v_vec_bcmk':v_save_bcmk, 'rwd_bcmk' : reward_save_bcmk, 'input_bcmk': PDE_input_save,'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Controller\n",
    "\n",
    "Paste from enjoy.py. file\n",
    "\n",
    "This is different from above as the environemnt is VECTORIZED.. So you may need to restart if you want to simulate above code again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "# workaround to unpickle olf model files\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from a2c_ppo_acktr.envs import VecPyTorch, make_vec_envs, make_vec_envs_arz\n",
    "from a2c_ppo_acktr.utils import get_render_func, get_vec_normalize\n",
    "\n",
    "import ipdb #SHP \n",
    "import gym_arz #SHP\n",
    "from settings_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"./trained_models/ppo\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## 1.Control Outlet, fix inlet\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_results/1_Outlet_Boundary_Results\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## 2.Control inlet, fix outlet \n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"./0914_inlet_controls/2020-09-14-15-52\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## 3.Control inlet & outlet \n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"./0915_inlet_outlet_controls/2020-09-16-10-33\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## 4.Control outlet w/ random r_s\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_trained_results/4_Stochastic_Outlet_Boundary_Results\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## 5.Control inlet w/ random r_s\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_trained_results/5_Stochastic_Inlet_Boundary_Results\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## Optional.Validation of M 40\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_trained_results/Response_complexity_M_40_Case\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## Optional.Validation of M 50\n",
    "# args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_trained_results/Response_complexity_M_50_Case\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "## Optional.Validation of M 60\n",
    "args = SimpleNamespace(env_name=\"arz-v0\", load_dir=\"save_trained_results/Response_complexity_M_60_Case\", seed = 1, det = True, non_det = False, log_interval=10)\n",
    "\n",
    "\n",
    "# args.env_name = \"arz-v0\"\n",
    "# args.load_dir = \"./trained_models/ppo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_settings['Scenario'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vec = make_vec_envs_arz(\n",
    "    args.env_name,\n",
    "    settings,\n",
    "    control_settings,\n",
    "    args.seed + 1000,\n",
    "    1,\n",
    "    None,\n",
    "    None,\n",
    "    device='cpu',\n",
    "    allow_early_resets=False)\n",
    "\n",
    "\n",
    "# Get a render function\n",
    "render_func = get_render_func(env_vec)\n",
    "\n",
    "# We need to use the same statistics for normalization as used in training\n",
    "\n",
    "\n",
    "## 1.Control Outlet, fix inlet\n",
    "# actor_critic, ob_rms = \\\n",
    "#             torch.load(os.path.join(args.load_dir, args.env_name + \"-tr-920-th\" + \".pt\"),map_location='cpu')\n",
    "\n",
    "## 2.Control inlet, fix outlet controller\n",
    "# actor_critic, ob_rms = \\\n",
    "#             torch.load(os.path.join(args.load_dir, args.env_name + \"-tr-2080-th\" + \".pt\"),map_location='cpu')\n",
    "\n",
    "## 3.Control inlet & outlet \n",
    "# actor_critic, ob_rms = \\\n",
    "#             torch.load(os.path.join(args.load_dir, args.env_name + \"-tr-2080-th\" + \".pt\"),map_location='cpu')\n",
    "\n",
    "## 4.Control outlet w/ random r_s\n",
    "# actor_critic, ob_rms = \\\n",
    "#             torch.load(os.path.join(args.load_dir, args.env_name + \"-tr-1040-th\" + \".pt\"),map_location='cpu')\n",
    "\n",
    "## 5.Control inlet w/ random r_s\n",
    "actor_critic, ob_rms = \\\n",
    "            torch.load(os.path.join(args.load_dir, args.env_name + \"-tr-1040-th\" + \".pt\"),map_location='cpu')\n",
    "\n",
    "\n",
    "vec_norm = get_vec_normalize(env_vec)\n",
    "if vec_norm is not None:\n",
    "    vec_norm.eval()\n",
    "    vec_norm.ob_rms = ob_rms\n",
    "\n",
    "recurrent_hidden_states = torch.zeros(1,\n",
    "                                      actor_critic.recurrent_hidden_state_size)\n",
    "masks = torch.zeros(1, 1)\n",
    "\n",
    "obs = env_vec.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.det = True\n",
    "\n",
    "## Added by SHP\n",
    "obs_save = []\n",
    "rwd_save = []\n",
    "PPO_input_save = []\n",
    "PPO_RHO_SAVE = []\n",
    "PPO_VEL_SAVE = []\n",
    "\n",
    "TIME_VEC = []\n",
    "dt = settings['dt']\n",
    "\n",
    "# Intial Condtions\n",
    "obs = env_vec.reset()\n",
    "obs_save.append(obs)\n",
    "\n",
    "PPO_RHO_SAVE.append((obs[0][0:51]*env.rs_desired+env.rs_desired).tolist())\n",
    "PPO_VEL_SAVE.append((obs[0][51:]*env.vs_desired+env.vs_desired).tolist())\n",
    "\n",
    "\n",
    "ACTION_VEC = []\n",
    "ACTION_VEC.append(np.array([0,0]))\n",
    "\n",
    "\n",
    "tt = 0\n",
    "TIME_VEC.append(tt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# while True:\n",
    "for i in range(N-1):\n",
    "    with torch.no_grad():\n",
    "        value, action, _, recurrent_hidden_states = actor_critic.act(\n",
    "            obs, recurrent_hidden_states, masks, deterministic=args.det)\n",
    "        \n",
    "    action = action[0]\n",
    "\n",
    "    PPO_input_save.append(action)\n",
    "    \n",
    "    obs, reward, done, _ = env_vec.step(action)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        print('DONE')\n",
    "        break\n",
    "    \n",
    "    ## Append Observation & Reward\n",
    "    obs_save.append(obs) \n",
    "    rwd_save.append(reward) \n",
    "              \n",
    "    ## Vectorized version\n",
    "    PPO_RHO_SAVE.append((obs[0][0:51]*env.rs_desired+env.rs_desired).tolist())\n",
    "    PPO_VEL_SAVE.append((obs[0][51:]*env.vs_desired+env.vs_desired).tolist())\n",
    "    \n",
    "    ## Append Action\n",
    "    ACTION_VEC.append(action.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    ## \n",
    "    tt += dt\n",
    "    TIME_VEC.append(tt)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_save_RL = np.array(PPO_RHO_SAVE)\n",
    "r_save_RL = r_save_RL.T\n",
    "\n",
    "v_save_RL = np.array(PPO_VEL_SAVE)\n",
    "v_save_RL = v_save_RL.T\n",
    "\n",
    "action_save_RL = np.array(ACTION_VEC)\n",
    "reward_save_RL = rwd_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L']  #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [sec]')\n",
    "ax.set_zlabel('Density [veh/km]')\n",
    "ax.set_title(r'$\\rho$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],r_save_RL[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=0.1,antialiased=False,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],r_save_RL[:,0],color='blue',LineWidth=4)\n",
    "ax.plot(xx[-1,:],tt[-1,:],r_save_RL[-1,:],color='red',LineWidth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L'] #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Position x [m]')\n",
    "ax.set_ylabel('Time [min]')\n",
    "ax.set_zlabel('Velocity [m/s]')\n",
    "ax.set_title(r'$V$')\n",
    "ax.plot_surface(xx[0:,0:],tt[0:,0:],v_save_RL[0:,0:],cmap=plt.cm.gray,edgecolors='#000000',linewidth=1,antialiased=True,rstride=1,cstride=100)\n",
    "ax.plot(xx[:,0],tt[:,0],v_save_RL[:,0],color='blue',LineWidth=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO REWARD PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "rwd_save_PPO = np.array(reward_save_RL)\n",
    "\n",
    "plt.plot(TIME_VEC[:-1],reward_save_RL)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('PPO Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rwd_save_PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_save_RL = r_save_RL * v_save_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.plot(r_save_RL[-1,:])\n",
    "plt.ylabel('r(L,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_save_RL[-1,:]*v_save_RL[-1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_RL[-1,:])\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_RL[0,:])\n",
    "plt.ylabel('q(0,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_RL[-2,:])\n",
    "plt.ylabel('q(L-1,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(q_save_RL[-1,:])\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.xlabel('Time [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_save_RL[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_save_RL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(action_save_RL[1:-1,1])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('q(L,t)')\n",
    "plt.title('INPUT SHAPE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,4))\n",
    "\n",
    "plt.plot(action_save_RL[1:-1,0])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('q(0,t)')\n",
    "plt.title('INPUT SHAPE')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('ARZ_python_result_RL_Outlet_M_60.mat',{'r_vec_RL':r_save_RL, 'v_vec_RL':v_save_RL, 'rwd_RL' : reward_save_RL, 'input_RL': action_save_RL, 'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//////////////////////////////////\n",
    "여기까지 진행함. 2020.05.18\n",
    "/////////////////////////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use 'PPO_input' to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "obs_save = []\n",
    "rwd_save = []\n",
    "\n",
    "\n",
    "for i in range(len(PPO_input_save)):\n",
    "    obs, reward, done, _ = env.step(PPO_input_save[i][0][0].numpy())\n",
    "    obs_save.append(obs)\n",
    "    rwd_save.append(reward)\n",
    "    \n",
    "    \n",
    "plt.plot(rwd_save)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDE BackStepping REWARD PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,reward_save,label='Benchmark')\n",
    "plt.plot(t[:-1],rwd_save, label='PPO')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Comparison')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO INPUT PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.plot(t[:-1],PPO_input_save)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('qs_input')\n",
    "plt.title('PPO INPUT')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDE BackStepping INPUT PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.plot(t[:-1],PDE_input_save[0,:-1])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('qs_input')\n",
    "plt.title('Benchmark INPUT')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how state looks like over time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting r_save, v_save for plot\n",
    "PPO_r_save_plt = np.zeros([51,len(PPO_r_save)])\n",
    "PPO_v_save_plt = np.zeros([51,len(PPO_v_save)])\n",
    "\n",
    "\n",
    "for i in range(len(PPO_r_save)):\n",
    "    PPO_r_save_plt[:,i] = PPO_r_save[i].numpy().reshape([51,])*rs + rs\n",
    "    PPO_v_save_plt[:,i] = PPO_v_save[i].numpy().reshape([51,])*vs + vs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(PPO_r_save_plt[:,0])\n",
    "# plt.plot(r_save[:,0])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('r')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(PPO_r_save_plt[:,0])\n",
    "plt.plot(r_save[:,0])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('r')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L']  #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx,tt,PPO_r_save_plt*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "## create meshgrid\n",
    "L = settings['L']  #[m]\n",
    "dx = settings['dx']\n",
    "x = np.arange(0,L+dx,dx)\n",
    "t = np.arange(0,T+dt,dt)\n",
    "\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xx,tt,PPO_v_save_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as sio\n",
    "# sio.savemat('ARZ_python_result_PPOsave.mat',{'r_vec_PPO':PPO_r_save_plt, 'v_vec_PPO':PPO_v_save_plt, 'rwd_PPO' : rwd_save, 'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is not used.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Smooth rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# parameters\n",
    "smoothing_window = 10\n",
    "\n",
    "# Smooth\n",
    "rewards_smth = pd.Series(rwd_save).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "\n",
    "# Plot\n",
    "plt.plot(t[:-1],rewards_smth)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Smoothed Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rewards_smth[9:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Smooth actions and use them to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_save = np.zeros([r_size,N])\n",
    "y_save = np.zeros([y_size,N])\n",
    "v_save = np.zeros([v_size,N])\n",
    "action_save = np.zeros([env.action_space.shape[0],N-1])\n",
    "reward_save = np.zeros(N)\n",
    "\n",
    "states = env.reset()\n",
    "r_save[:,0] = env.r.reshape(r_size,)\n",
    "y_save[:,0] = env.y.reshape(y_size,)\n",
    "v_save[:,0] = env.v.reshape(v_size,)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N-1):\n",
    "    action = agent.compute_action(states)\n",
    "    states, reward, is_done, info = env.step(action)\n",
    "    r_save[:,i+1] = env.r.reshape(r_size,)\n",
    "    y_save[:,i+1] = env.y.reshape(y_size,)\n",
    "    v_save[:,i+1] = env.v.reshape(v_size,)\n",
    "    reward_save[i+1] = reward\n",
    "    action_save[:,i] = action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the action and replay.\n",
    "\n",
    "# parameters\n",
    "smoothing_window = 10\n",
    "\n",
    "# Smooth\n",
    "action_smth = pd.Series(PPO_input_save).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "\n",
    "for i in range(smoothing_window):\n",
    "    action_smth[i]=action_smth[smoothing_window]\n",
    "\n",
    "# Plot\n",
    "plt.plot(t[:-1], action_smth)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Action_idx')\n",
    "plt.title('Smooth Action')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_smth[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "## Added by SHP\n",
    "obs_smth_save = []\n",
    "rwd_smth_save = []\n",
    "PPO_input_save = []\n",
    "# ipdb.set_trace()\n",
    "obs_save.append(obs)\n",
    "\n",
    "\n",
    "if args.env_name.find('Bullet') > -1:\n",
    "    import pybullet as p\n",
    "\n",
    "    torsoId = -1\n",
    "    for i in range(p.getNumBodies()):\n",
    "        if (p.getBodyInfo(i)[0].decode() == \"torso\"):\n",
    "            torsoId = i\n",
    "\n",
    "for i in range(len(action_smth)):\n",
    "\n",
    "    # Obser reward and next obs\n",
    "    obs, reward, done, _ = env.step(action_smth[i])\n",
    "    obs_smth_save.append(obs) # SHP\n",
    "    rwd_smth_save.append(reward) # SHP\n",
    "\n",
    "    masks.fill_(0.0 if done else 1.0)\n",
    "\n",
    "    if args.env_name.find('Bullet') > -1:\n",
    "        if torsoId > -1:\n",
    "            distance = 5\n",
    "            yaw = 0\n",
    "            humanPos, humanOrn = p.getBasePositionAndOrientation(torsoId)\n",
    "            p.resetDebugVisualizerCamera(distance, yaw, -20, humanPos)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "    ## Commented by SHP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL-Lib version [NOT USED HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp=agent.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.plot(t,reward_save)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(reward_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "sio.savemat('ARZ_python_result_RL.mat',{'r_vec_RL':r_save, 'v_vec_RL':v_save, 'rwd_RL' : reward_save, 'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth\n",
    "\n",
    "2019.01.31 \n",
    "- 1) Using Pandas, smooth the curve and re-use the input see resutls.\n",
    "- 2) Moving average method, see the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Smooth rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# parameters\n",
    "smoothing_window = 10\n",
    "\n",
    "# Smooth\n",
    "rewards_smth = pd.Series(reward_save).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "\n",
    "# Plot\n",
    "plt.plot(t,rewards_smth)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Smoothed Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Smooth actions and use them to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_save = np.zeros([r_size,N])\n",
    "y_save = np.zeros([y_size,N])\n",
    "v_save = np.zeros([v_size,N])\n",
    "action_save = np.zeros([env.action_space.shape[0],N-1])\n",
    "reward_save = np.zeros(N)\n",
    "\n",
    "states = env.reset()\n",
    "r_save[:,0] = env.r.reshape(r_size,)\n",
    "y_save[:,0] = env.y.reshape(y_size,)\n",
    "v_save[:,0] = env.v.reshape(v_size,)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N-1):\n",
    "    action = agent.compute_action(states)\n",
    "    states, reward, is_done, info = env.step(action)\n",
    "    r_save[:,i+1] = env.r.reshape(r_size,)\n",
    "    y_save[:,i+1] = env.y.reshape(y_size,)\n",
    "    v_save[:,i+1] = env.v.reshape(v_size,)\n",
    "    reward_save[i+1] = reward\n",
    "    action_save[:,i] = action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the action\n",
    "action_save_1d = action_save.flatten('C') \n",
    "plt.plot(t[:-1], action_save_1d)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Action_idx')\n",
    "plt.title('Action')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth the action and replay.\n",
    "\n",
    "# parameters\n",
    "smoothing_window = 10\n",
    "\n",
    "# Smooth\n",
    "action_smth = pd.Series(action_save_1d).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "\n",
    "for i in range(smoothing_window):\n",
    "    action_smth[i]=action_smth[smoothing_window]\n",
    "\n",
    "# Plot\n",
    "plt.plot(t[:-1], action_smth)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Action_idx')\n",
    "plt.title('Smooth Action')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run\n",
    "\n",
    "r_save = np.zeros([r_size,N])\n",
    "y_save = np.zeros([y_size,N])\n",
    "v_save = np.zeros([v_size,N])\n",
    "reward_save = np.zeros(N)\n",
    "\n",
    "states = env.reset()\n",
    "r_save[:,0] = env.r.reshape(r_size,)\n",
    "y_save[:,0] = env.y.reshape(y_size,)\n",
    "v_save[:,0] = env.v.reshape(v_size,)\n",
    "\n",
    "for i in range(N-1):\n",
    "    action = action_smth[i]\n",
    "    states, reward, is_done, info = env.step(action)\n",
    "    r_save[:,i+1] = env.r.reshape(r_size,)\n",
    "    y_save[:,i+1] = env.y.reshape(y_size,)\n",
    "    v_save[:,i+1] = env.v.reshape(v_size,)\n",
    "    reward_save[i+1] = reward\n",
    "    action_save[:,i] = action\n",
    "\n",
    "# Plot\n",
    "plt.plot(t,reward_save)\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward')\n",
    "plt.grid(True)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import scipy.io as sio\n",
    "xx, tt = np.meshgrid(x,t,indexing='ij')\n",
    "sio.savemat('ARZ_python_result_RL_smth_rand_tau.mat',{'r_vec_RL':r_save, 'v_vec_RL':v_save, 'rwd_RL' : reward_save, 'xx': xx, 'tt' : tt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
